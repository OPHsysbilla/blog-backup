---
title: Linux面试答案整理
date: 2020-11-17 15:34:56
tags:
    - 面试

---
# 异步I/O
见从中断讲起的好文：[epoll 的本质是什么？](https://my.oschina.net/editorial-story/blog/3052308?p=2)
linux的网络IO中是不存在异步IO的，linux的网络IO处理的第二阶段总是阻塞等待数据copy完成的。真正意义上的网络异步IO是Windows下的IOCP（IO完成端口）模型
<!--more-->

![各种I/O模型](https://blog-10039692.file.myqcloud.com/1500017105443_4641_1500017105783.png)
> non-blocking IO仅仅要求处理的第一阶段不block即可，而asynchronous IO要求两个阶段都不能block住
# select、poll、epoll之间的区别
`select` ，`poll` ，`epoll` 都是**I/O多路复用**的机制，都是同步I/O。都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是**阻塞**的。好处在于可以以较少的代价来同时监听处理多个IO。
> **异步I/O**则无需自己负责进行读写，其实现会负责*把数据从内核拷贝到用户空间*。  
`epoll`是`Linux`所特有，而`select`则应该是`POSIX`所规定，一般操作系统均有实现
1. `select` -> 时间复杂度`O(n)`
   - 具有O(n)的无差别轮询复杂度，同时处理的流越多无差别轮询时间就越长。
   - > 仅仅知道了有I/O事件发生了，却并不知道是哪那几个流。只能无差别轮询所有流，找出能读/写的流。
   - 通过设置或者检查存放fd标志位的数据结构来处理。单个进程能监听的fd数量有限。
   - > 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，默认1024
   - 需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大
2. `poll` -> 时间复杂度`O(n)`
`poll` 本质上和`select`没有区别。使用`pollfd`结构而不是select的`fd_set`结构
   - 消息传递需要内核拷贝。将用户传入的数组拷贝到**内核空间**，查询每个`fd`对应的设备状态数组
   - > 如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程。直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。
   - **没有最大连接数的限制**，原因是它是基于**链表**来存储的。缺点：
   - 1. 大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。
   - 2. poll如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

3. `epoll` -> 时间复杂度`O(1)`
`epoll`可以理解为**event poll**，不同于忙轮询和无差别轮询
poll实际上是**事件驱动**（每个事件关联上fd）的，哪个流发生了I/O只回调哪个流

- 通过内核和用户空间共享一块内存来实现的。利用`mmap()`文件映射内存加速与内核空间的消息传递
- 连接数上限很大，1G内存的机器上可以打开10万左右的连接。（通过`cat /proc/sys/fs/file-max`可查看）
- `epoll`每次注册新的事件到`epoll`句柄中时（在`epoll_ctl`中指定`EPOLL_CTL_ADD`），保证了每个`fd`在整个过程中只会拷贝一次。
- > 在活跃`socket`较少的情况下，使用`epoll`没有前面两者的线性下降的性能问题，但是所有`socket`都很活跃的情况下，可能会有性能问题

epoll有它的使用场景，更适合于处理高并发的场合。
如果并发量很少，比如几百个连接这个级别，select的效率会比epoll高只有连接数大到一定长度，epoll的额外消耗才能抵消select的遍历

游戏服务器一般是`one loop per thread`
多线程服务器此文 [陈硕的 Blog](https://www.cnblogs.com/solstice/archive/2010/02/12/multithreaded_server.html)